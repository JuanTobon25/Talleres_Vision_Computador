{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmuz9gFuRSq9",
        "outputId": "2bfb2808-a7ad-4779-fa99-e27795c70b42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_TYPE = \"cnn\"   # \"cnn\" or \"vit\"\n",
        "BATCH_SIZE = 128\n",
        "IMG_SIZE = 64        # Tiny ImageNet images are 64x64\n",
        "EPOCHS = 12          # prueba corta; aumentar si tienes tiempo\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ],
      "metadata": {
        "id": "dnCtg8OCRY7T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tiny ImageNet (stanford) - contiene train/val/test\n",
        "DATA_DIR = Path(\"/content/tiny-imagenet-200\")\n",
        "if not DATA_DIR.exists():\n",
        "    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
        "    zip_path = \"/content/tiny-imagenet-200.zip\"\n",
        "    print(\"Descargando Tiny ImageNet (~240MB)...\")\n",
        "    !wget -q --show-progress -O {zip_path} {url}\n",
        "    print(\"Extrayendo...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        zf.extractall(\"/content\")\n",
        "    print(\"Listo.\")\n",
        "else:\n",
        "    print(\"Tiny ImageNet ya existe en\", DATA_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8zZ3gsMRbGU",
        "outputId": "1beacc53-d0e9-4854-8866-30abdf737330"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando Tiny ImageNet (~240MB)...\n",
            "/content/tiny-image 100%[===================>] 236.61M  19.8MB/s    in 8.0s    \n",
            "Extrayendo...\n",
            "Listo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Para la tarea de rotación vamos a tomar las imágenes y generar 4 rotaciones por imagen.\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "def list_image_paths(base_dir):\n",
        "    # listará imágenes de train (cada clase en subcarpeta) y val\n",
        "    train_dir = base_dir / \"train\"\n",
        "    val_dir = base_dir / \"val\"\n",
        "    train_paths = glob.glob(str(train_dir / \"*\"/ \"images\" / \"*.JPEG\"))\n",
        "    val_images = glob.glob(str(val_dir / \"images\" / \"*.JPEG\"))\n",
        "    # También hay validation annotations para labels, pero para pretexto no necesitamos labels\n",
        "    return train_paths, val_images\n",
        "\n",
        "train_paths, val_paths = list_image_paths(DATA_DIR)\n",
        "len(train_paths), len(val_paths)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jmhGs48RdTa",
        "outputId": "304ba727-4f56-4e09-9cf5-effcae10ac07"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(path, img_size=IMG_SIZE):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [img_size, img_size])\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "# Augmentaciones inspiradas en el artículo (random crop, flip, color jitter, gaussian noise)\n",
        "def random_augment(image):\n",
        "    # Random crop + resize (scale + crop)\n",
        "    image = tf.image.random_crop(tf.image.pad_to_bounding_box(image, 4, 4, IMG_SIZE+8, IMG_SIZE+8), size=[IMG_SIZE, IMG_SIZE, 3])\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    # Color jitter: brightness, contrast, saturation, hue\n",
        "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
        "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
        "    image = tf.image.random_saturation(image, 0.8, 1.2)\n",
        "    image = tf.image.random_hue(image, 0.05)\n",
        "    # Clip\n",
        "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
        "    # Optional gaussian noise\n",
        "    noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=0.02)\n",
        "    image = image + noise\n",
        "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
        "    return image\n",
        "\n",
        "def make_rotations(image):\n",
        "    # Return 4 images and labels 0..3 corresponding to rotation * 90 degrees\n",
        "    imgs = [\n",
        "        image,\n",
        "        tf.image.rot90(image, k=1),\n",
        "        tf.image.rot90(image, k=2),\n",
        "        tf.image.rot90(image, k=3),\n",
        "    ]\n",
        "    labels = [0,1,2,3]\n",
        "    return imgs, labels\n",
        "\n",
        "# Dataset generator that yields (image, rotation_label)\n",
        "def dataset_from_paths(paths, batch_size=BATCH_SIZE, training=True):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(paths)\n",
        "    if training:\n",
        "        ds = ds.shuffle(10000, reshuffle_each_iteration=True)\n",
        "    ds = ds.map(lambda p: preprocess_image(p), num_parallel_calls=AUTOTUNE)\n",
        "    if training:\n",
        "        ds = ds.map(lambda x: random_augment(x), num_parallel_calls=AUTOTUNE)\n",
        "    # expand each image into 4 rotated examples\n",
        "    def expand_rotations(img):\n",
        "        imgs, labs = make_rotations(img)\n",
        "        return tf.data.Dataset.from_tensor_slices((tf.stack(imgs), tf.constant(labs, dtype=tf.int32)))\n",
        "    ds = ds.flat_map(expand_rotations)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = dataset_from_paths(train_paths, training=True)\n",
        "val_ds = dataset_from_paths(val_paths, training=False)\n"
      ],
      "metadata": {
        "id": "YAvta-pWRg5J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 6 - Modelos: CNN vanilla y ViT simple\n",
        "# Encoder outputs a feature vector; final head predicts 4 rotations.\n",
        "\n",
        "def build_cnn_encoder(input_shape=(IMG_SIZE,IMG_SIZE,3), embedding_dim=256):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPool2D(2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPool2D(2)(x)\n",
        "\n",
        "    x = layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPool2D(2)(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(embedding_dim, activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    model = keras.Model(inputs, x, name=\"cnn_encoder\")\n",
        "    return model\n",
        "\n",
        "# Simple Vision Transformer (patchify + transformer blocks)\n",
        "def build_vit_encoder(input_shape=(IMG_SIZE,IMG_SIZE,3), patch_size=8, num_patches=None,\n",
        "                      projection_dim=64, transformer_layers=4, num_heads=4, mlp_dim=128):\n",
        "    if num_patches is None:\n",
        "        num_patches = (input_shape[0] // patch_size) * (input_shape[1] // patch_size)\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    # Create patches\n",
        "    patches = layers.Conv2D(filters=projection_dim, kernel_size=patch_size, strides=patch_size, padding='valid')(inputs)\n",
        "    # patches shape: (B, H/ps, W/ps, projection_dim) --> flatten to sequence\n",
        "    shape = tf.shape(patches)\n",
        "    x = layers.Reshape((num_patches, projection_dim))(patches)\n",
        "    # Add positional embeddings\n",
        "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
        "    pos_emb = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)(positions)\n",
        "    x = x + pos_emb\n",
        "    # Transformer blocks\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer norm + MultiHead\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim)(x1, x1)\n",
        "        x = layers.Add()([attn, x])\n",
        "        # MLP\n",
        "        x2 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        mlp = layers.Dense(mlp_dim, activation='relu')(x2)\n",
        "        mlp = layers.Dense(projection_dim)(mlp)\n",
        "        x = layers.Add()([mlp, x])\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    model = keras.Model(inputs, x, name=\"vit_encoder\")\n",
        "    return model\n",
        "\n",
        "# Build final model depending on MODEL_TYPE\n",
        "if MODEL_TYPE == \"cnn\":\n",
        "    encoder = build_cnn_encoder()\n",
        "elif MODEL_TYPE == \"vit\":\n",
        "    encoder = build_vit_encoder()\n",
        "else:\n",
        "    raise ValueError(\"MODEL_TYPE must be 'cnn' or 'vit'\")\n",
        "\n",
        "# Full model: encoder + rotation prediction head\n",
        "inputs = keras.Input(shape=(IMG_SIZE,IMG_SIZE,3))\n",
        "z = encoder(inputs)\n",
        "z = layers.Dropout(0.3)(z)\n",
        "outputs = layers.Dense(4, activation='softmax')(z)\n",
        "model = keras.Model(inputs, outputs, name=f\"{MODEL_TYPE}_rotation_model\")\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "lAJYGW_vRkb0",
        "outputId": "99cdbaff-759a-4bf4-e1a3-42a7a35b9c94"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"cnn_rotation_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cnn_rotation_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ cnn_encoder (\u001b[38;5;33mFunctional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m439,424\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │         \u001b[38;5;34m1,028\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ cnn_encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">439,424</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,028</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m440,452\u001b[0m (1.68 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">440,452</span> (1.68 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m439,044\u001b[0m (1.67 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">439,044</span> (1.67 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,408\u001b[0m (5.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> (5.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 7 - Compilar y callbacks\n",
        "lr = 1e-3\n",
        "optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "checkpoint_path = f\"/content/{MODEL_TYPE}_rotation_checkpoint.h5\"\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_accuracy'),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6),\n",
        "    keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=6, restore_best_weights=True)\n",
        "]\n"
      ],
      "metadata": {
        "id": "lJLqgSYcRnMu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 8 - Entrenamiento\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDkwVk4ERsxn",
        "outputId": "e125ffaa-57dc-4da5-a7a0-eea17a3c3296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "    193/Unknown \u001b[1m805s\u001b[0m 4s/step - accuracy: 0.3455 - loss: 1.5748"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 9 - Evaluación rápida y guardar encoder\n",
        "print(\"Mejor accuracy val:\", max(history.history.get(\"val_accuracy\", [0])))\n",
        "# Guardar encoder separado (para transfer learning posterior)\n",
        "encoder.save(f\"/content/{MODEL_TYPE}_encoder_saved\")\n",
        "print(\"Encoder guardado en /content\")\n"
      ],
      "metadata": {
        "id": "SVtjHRdeRuDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 10 - Ejemplo de uso del encoder para transfer learning\n",
        "# Supongamos que ahora quieres usar encoder y agregar una cabeza para clasificación\n",
        "num_new_classes = 200  # en Tiny ImageNet hay 200 clases; esto sería un ejemplo\n",
        "inputs = keras.Input(shape=(IMG_SIZE,IMG_SIZE,3))\n",
        "features = encoder(inputs)\n",
        "x = layers.Dense(512, activation='relu')(features)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "out = layers.Dense(num_new_classes, activation='softmax')(x)\n",
        "transfer_model = keras.Model(inputs, out, name=\"transfer_model\")\n",
        "\n",
        "# Congelar encoder y compilar\n",
        "encoder.trainable = False\n",
        "transfer_model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
        "                       loss='sparse_categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "transfer_model.summary()\n",
        "# Para entrenar necesitarás etiquetas de Tiny ImageNet (esta celda es demo)\n"
      ],
      "metadata": {
        "id": "2uxGBLKiRu84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J5A72tHNRwn6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}